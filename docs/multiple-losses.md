Great. I’ll explore the theoretical and practical tradeoffs between additive and multiplicative loss combinations in multi-objective neural network optimization. This will include an analysis of gradient dynamics, examples from computer vision and other fields, numerical stability concerns, and modern strategies for balancing losses, especially in PyTorch implementations.

I’ll also investigate hybrid approaches like geometric or harmonic mean loss combinations and discuss their effects on training stability and convergence.

I’ll let you know once I have a comprehensive summary ready.


# Combining Multiple Losses: Additive vs Multiplicative Tradeoffs

## Introduction

Training neural networks with multiple loss components is common in multi-objective and multi-task learning. A typical approach is to form a **weighted sum of losses** (e.g. \$L\_{\text{total}} = \alpha L\_1 + \beta L\_2 + \gamma L\_3\$), where hyperparameters \$\alpha,\beta,\gamma\$ control the contribution of each loss. An alternative is to combine losses **multiplicatively** (e.g. \$L\_{\text{total}} = L\_1 \times L\_2 \times L\_3\$). This report examines the theoretical and practical tradeoffs between these approaches, focusing on a generative vision task that uses three loss terms – a VGG perceptual loss, an \$L\_1\$ pixel loss, and an LPIPS perceptual similarity loss – to render images with triangles. Key considerations include differences in gradient dynamics, successful use cases of multiplicative losses, numerical stability issues, hybrid combination strategies (geometric or harmonic means), and modern techniques for loss balancing (e.g. GradNorm, uncertainty-based weighting). Throughout, we provide practical insights (especially for PyTorch implementations) and examples from computer vision and beyond.

## Additive vs. Multiplicative Loss Combination

**Additive (Weighted Summation) Approach:** Combining losses by weighted addition is the standard practice in deep learning. It is simple to implement (`total_loss = alpha*L1 + beta*L2 + ...` in PyTorch) and treats each loss as an independently scaled objective. However, choosing appropriate weights is non-trivial – different loss terms often have different units and scales (e.g. cross-entropy measured in nats vs. \$L\_2\$ measured in squared pixel intensities). Without careful weighting, one loss can dominate the training objective purely due to scale. Manually tuning these weights is difficult and expensive. In practice, practitioners often set weights via heuristics (e.g. normalize each loss by its initial value or expected range) or grid-search. Additive combination also assumes we are content with a linear trade-off: the optimizer can sacrifice improvements in one loss for gains in another according to the fixed weights. This linear blending may not adequately enforce that *all* objectives improve – the model might minimize the weighted sum by focusing on only a subset of losses (especially those that are easier to optimize), neglecting others if their weight is low. On the positive side, gradients from an additive loss are a simple weighted sum of individual gradients:

$\nabla_\theta L_{\text{sum}} = \alpha\,\nabla_\theta L_1 + \beta\,\nabla_\theta L_2 + \gamma\,\nabla_\theta L_3.$

This means each loss contributes independently to the parameter updates, scaled by a constant factor. It’s straightforward to reason about: increasing \$\alpha\$ amplifies the influence of \$L\_1\$’s gradient, and so on. There is no direct interaction term between losses in the gradient – their interplay is only through the shared network parameters.

**Multiplicative (Product) Approach:** In a multiplicative loss, the total objective is the *product* of the components (or an equivalent formulation like a geometric mean). For example, with three losses one could use \$L\_{\text{total}} = L\_1 \cdot L\_2 \cdot L\_3\$. Intuitively, this strategy **forces the model to perform well on all losses simultaneously** – the product is low (good) only if *every* \$L\_i\$ is small. One immediate appeal of this approach is that it sidesteps the issue of adding quantities with different units; multiplying losses yields an objective with “meaningful” combined units without an arbitrary weighting to make units commensurate. Indeed, some researchers have proposed that multiplying losses could remove the need to manually tune loss weights in cases where all \$L\_i\$ share the same importance. Another connection is probabilistic: if each \$L\_i\$ corresponded to a negative log-likelihood term, then summing them (as is standard) is equivalent to multiplying the underlying likelihoods – so a product of losses can be seen as a kind of “joint” objective. In multi-label classification, for example, independent label likelihoods multiply, while their negative log-losses add. This hints that under certain assumptions, a multiplicative loss might align with maximizing a joint probability of achieving good performance on all tasks.

Despite these motivations, **multiplicative losses are not commonly used** in practice. They introduce complex gradient dynamics and stability issues, as detailed next. The gradient of a product involves *cross-terms*: using the example \$L = L\_1 L\_2\$ for simplicity,

$\nabla_\theta L = L_2\,\nabla_\theta L_1 \;+\; L_1\,\nabla_\theta L_2.$

By extension, for three losses \$L = L\_1 L\_2 L\_3\$, each term’s gradient is scaled by the product of the *other* two losses:

$\nabla_\theta L_1 = L_2 L_3\,\nabla_\theta L_1,$
$\nabla_\theta L_2 = L_1 L_3\,\nabla_\theta L_2,$
$\nabla_\theta L_3 = L_1 L_2\,\nabla_\theta L_3,$

so \$\nabla\_\theta L = L\_2L\_3,\nabla\_\theta L\_1 + L\_1L\_3,\nabla\_\theta L\_2 + L\_1L\_2,\nabla\_\theta L\_3\$. This coupling has important consequences:

* **Gradient Dynamics and Task Balancing:** In the product formulation, the **magnitude of each loss directly influences the gradient contributions of the others**. If one loss is very large, it will *increase* the gradient magnitude coming from other losses (since those gradients are multiplied by this large value). This tends to put *extra pressure* on reducing the currently large error term. Conversely, if one loss is extremely small, it will *down-weight* the gradients of the other losses (since those terms are multiplied by the small value). In effect, the product loss automatically allocates more gradient focus to criteria where error remains high, which can be seen as an *implicit balancing*. Researchers have noted that this property can prioritize learning all tasks more equally: *“the definition \[geometric mean of losses] makes sure that all tasks are making progress”*. Empirical studies in multi-task models have indeed found that a product/geometric-mean loss can prevent the network from just optimizing the “easy” task and ignoring harder ones, an issue sometimes seen with naive loss summation. In one report on a unified model for vision tasks, using the product of losses yielded significantly better accuracy on *all* tasks compared to a weighted sum (equal weights) baseline. The model learned multiple outputs (segmentation, depth, motion) and the product-based loss achieved a more balanced, improved performance than the sum-based loss, demonstrating the capacity of the unified model to learn all tasks with similar accuracy.

* **Conflict vs. Synergy:** With additive losses, if the gradients of two tasks conflict (point in opposite directions in parameter space), the resulting update is their weighted sum – one can partially cancel the other. There is no inherent mechanism to resolve such conflict besides adjusting weights or using specialized gradient handling algorithms. Multiplicative losses intertwine the objectives, which can sometimes mitigate conflict by the above-mentioned balancing effect (focusing on whichever loss is currently worse). However, conflicts can still occur – the product rule doesn’t magically align the gradients, it just scales them. In worst cases, if two losses are at odds, the product approach might lead to oscillation: when \$L\_1\$ is large, the model pushes hard on \$L\_2\$ (since \$L\_1\$ amplifies \$\nabla L\_2\$), and vice versa, potentially see-sawing. This behavior is complex and task-dependent. There isn’t a guarantee that product combining always improves synergy; it simply changes how the competition between losses is mediated.

* **Vanishing Gradients Near Optimum:** A known downside of multiplicative loss is the **vanishing gradient problem when all losses become small**. If each \$L\_i\$ is near zero (as we hope when the model is close to optimal on all objectives), their product \$L\_1L\_2\cdots L\_n\$ becomes *extremely* small – potentially much smaller than any individual loss. The gradient magnitude, being proportional to that product, also becomes tiny. In other words, as the model approaches a good solution, the product loss’s gradients *shrink faster* than an additive loss’s gradients would, which can **stall learning** when nearing convergence. An additive loss does not suffer this particular issue: if \$L\_1\$ and \$L\_2\$ are both small (say 0.01 each), an additive loss might be 0.02 with gradients of order 0.01; a multiplicative loss would be 0.0001 with gradients of order 0.0001, possibly below machine precision or noise level. Thus, multiplicative loss can *exacerbate* convergence problems as the losses approach zero. One redditor experimenting with multiplying losses observed this vanishing gradient effect and concluded that “multiplying them exacerbates the vanishing gradients problem”.

* **Dominance of One Loss and Need for Weighting:** It’s important to dispel the notion that a multiplicative loss automatically eliminates the need for weighting. If one loss term is naturally much larger in magnitude than the others (or decreases much more slowly), it can still **dominate the product**. For example, if \$L\_1\$ remains relatively high, the product \$L\_1L\_2L\_3\$ will be high regardless of \$L\_2,L\_3\$, keeping pressure on reducing \$L\_1\$. Conversely, if one loss is orders of magnitude smaller than the rest, the product might become small enough to give a false sense of overall performance. As one commenter noted, *“if one output’s loss is several times bigger than the other, it would dominate the product (or log-sum), giving it more preference than intended”*. Without explicit weights, a product can thus suffer the **same imbalance issues as a sum**: the larger-scale loss steers the optimization. In practice, one can introduce exponents as weights in the product formulation (e.g. \$L = L\_1^{\alpha} L\_2^{\beta} L\_3^{\gamma}\$), which is equivalent to weighting the log-losses (discussed below). This brings back hyperparameters \$\alpha,\beta,\gamma\$ to tune – so the weighting problem is not truly eliminated, just transformed. The difference is that these exponents weight *relative importance in a multiplicative sense*, but selecting them may be just as hard as selecting additive weights.

The table below summarizes key differences between additive and multiplicative loss combinations:

| **Aspect**               | **Additive (Weighted Sum)**                                                                                                                                                                                                                         | **Multiplicative (Product/Geometric)**                                                                                                                                                                                                                                                                                     |
| ------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Weight Tuning**        | Requires choosing weights to balance different loss scales (often manually or via heuristics). Different units make direct summation arbitrary.                                                                                                     | Potentially reduces reliance on manual weights if all losses are equally important. However, may still need implicit weighting (e.g. via exponents or normalization) if one loss inherently dominates.                                                                                                                     |
| **Gradient Composition** | \$\nabla L = \sum\_i w\_i,\nabla L\_i\$. Gradients add linearly, no direct interaction between loss terms. Easy to implement and tune each term’s influence via \$w\_i\$.                                                                           | \$\nabla L\$ contains cross-terms (product rule). Each loss’s gradient is scaled by the product of the others, coupling their effects. A large \$L\_j\$ amplifies gradients of \$L\_i\neq j\$, a near-zero \$L\_j\$ suppresses others’ gradients.                                                                          |
| **Task Balancing**       | Fixed weights mean the trade-off is predetermined. The model might favor minimizing one loss if its weight is higher or if it can reduce that loss easily, possibly neglecting others. Requires re-weighting or advanced methods to ensure balance. | Tends to **prioritize all tasks**: the objective is low only if every \$L\_i\$ is low. Automatically increases focus on tasks with high error (since they keep the product high) and deprioritizes tasks already nearly solved. Can mitigate “one-task dominates” issues, up to a point.                                   |
| **Convergence Behavior** | No inherent vanishing gradient issue – each loss can keep pushing as long as it has gradient. Convergence depends on weight settings (bad weights can stall one task entirely if too low).                                                          | Suffers **vanishing gradients** when all losses are small. Training may stall near the optimum because the product shrinks faster than each individual loss. Also, if any single \$L\_i\$ reaches *exactly* 0 (perfect for one task), the product is 0 and gradients of others become 0 – training would stop prematurely. |
| **Numerical Stability**  | Generally stable; summing avoids extreme scales (unless weights or losses are very large). No risk of underflow/overflow from summation alone.                                                                                                      | More prone to **underflow** (product of many small losses can become extremely tiny, hitting floating-point limits) or **overflow** (product of large losses). Care needed to avoid numerical extremes. Typically implemented via log-sum to improve stability.                                                            |
| **Interpretation**       | Linear combination: can be interpreted as a weighted average of objectives. However, summing different units is hard to interpret quantitatively (weights essentially convert units).                                                               | Geometric combination: output units are the geometric mix of units, which some argue is more meaningful. Difficult to interpret magnitude, but zero product definitively means all losses are zero. Emphasizes *relative* performance across tasks (all must improve).                                                     |
| **Use in Practice**      | Most common approach in multi-loss or multi-task training. Well-supported by frameworks (just add losses). Requires tuning or using methods like uncertainty weighting, etc., to find good weights.                                                 | Rarely used directly, but appears in specific research contexts (e.g. style transfer, multi-task geometric losses). Often implemented via **sum of log-losses** (equivalent to product) to avoid vanishing gradients. Can be seen in “product-of-experts” models or as a special case of multi-objective optimization.     |

**Key Insight:** The additive approach offers simplicity and clear control over each loss’s influence, at the cost of requiring manual balance of incommensurable terms. The multiplicative approach inherently ties the fate of each loss to the others, which can encourage a more balanced optimization (no single term can be large if the objective is to be minimized). However, this tight coupling can introduce optimization difficulties, especially as losses shrink. In practice, additive losses with proper weighting (possibly dynamic weighting schemes) tend to be more stable, whereas multiplicative losses may need modifications (like taking logs) to be viable.

## Numerical Stability and Log-Sum Trick

When using a multiplicative loss, one must confront **numerical stability** challenges. Consider training with three loss terms where each term might be on the order of, say, \$10^{-1}\$ to \$10^0\$. The raw product could be on the order of \$10^{-3}\$, and as training progresses it could become much smaller (if each loss drops to 0.01, the product is \$1\times10^{-6}\$). In single-precision floating point, numbers smaller than \~\$10^{-38}\$ underflow to zero. Even far above that threshold, extremely small values can degrade gradient precision. On the flip side, if losses are large (imagine each around 10, product \~1000), the product could blow up and overflow if extreme. While overflow is less likely in practice (since loss functions usually diminish as training continues), underflow and loss of gradient resolution is a real concern.

**Log-Sum Implementation:** A common solution is to optimize a *sum of log-losses* instead of the direct product. If we define:

$\tilde{L} = \log(L_1 + \epsilon) + \log(L_2 + \epsilon) + \log(L_3 + \epsilon),$

this is mathematically equivalent to \$\log(L\_1 L\_2 L\_3)\$ (with a small \$\epsilon\$ added to each \$L\_i\$ to avoid \$\log(0)\$). Minimizing \$\tilde{L}\$ will minimize the product \$L\_1L\_2L\_3\$, but with much better numerical conditioning. The gradient \$\nabla\_\theta \tilde{L} = \frac{1}{L\_1}\nabla\_\theta L\_1 + \frac{1}{L\_2}\nabla\_\theta L\_2 + \frac{1}{L\_3}\nabla\_\theta L\_3\$ (for \$\epsilon\$ small) avoids the tiny multiplication of terms; instead, each component’s gradient is *inversely* scaled by its current loss magnitude. This drastically reduces the vanishing gradient issue: when a loss \$L\_i\$ is very small, \$\frac{1}{L\_i}\$ is large, so that task’s gradient gets *amplified* to ensure it doesn’t entirely stagnate. In the original product formulation, a small \$L\_i\$ would have instead *suppressed* gradients of other terms, possibly freezing progress on them – but in the log domain, a small \$L\_i\$ increases its own gradient contribution \$\frac{1}{L\_i}\nabla L\_i\$, helping drive it down to even smaller values (or maintaining pressure on it). Essentially, the log transform **trades multiplicative interactions for additive ones in log-space** and **stabilizes the gradients**. One practitioner noted that summing log-losses “eliminates the vanishing gradient problem” present in raw multiplication. In fact, using \$\tilde{L} = \sum\_i \log(L\_i)\$ is exactly the *geometric mean* loss approach (since \$\sum \log L\_i\$ is \$\log(\prod L\_i)\$). The geometric mean of losses \$= (\prod\_i L\_i)^{1/n}\$ has the same minimizer as the product, but it’s numerically nicer to work with the log-sum form.

It’s worth noting that while the sum-of-logs objective has the same minima as the product, it **changes the gradient dynamics** slightly due to the \$1/L\_i\$ factors. As mentioned, this makes the training focus relatively more on smaller-loss tasks (because their gradients get larger weight when \$L\_i\$ is small) and relatively less on larger-loss tasks (if \$L\_j\$ is huge, \$1/L\_j\$ is small, so its gradient is down-weighted). This is actually another form of **automatic balancing**: it ensures no single loss term goes to zero too early or lags far behind, because the weighting continuously adjusts. In fact, this idea is very similar to some dynamic re-weighting heuristics used in practice (discussed later). The **tradeoff** is that if one loss becomes extremely small compared to others, its gradient can become extremely large (due to \$1/L\_i\$), which might cause instability or oscillation. In implementation, one should therefore add a small constant \$\epsilon\$ inside the log (e.g. \$\epsilon = 1e-8\$) to cap the gradient magnification and to avoid undefined log at 0. PyTorch’s autodiff can handle the \$\log\$ smoothly as long as \$\epsilon\$ is there to prevent log(0).

**Practical Tip:** Implementing a log-sum loss in PyTorch is straightforward. For example, if `loss1`, `loss2` are your loss tensors, you can do:

```python
eps = 1e-8
total_loss = torch.log(loss1 + eps) + torch.log(loss2 + eps)
total_loss.backward()
```

This will correctly accumulate gradients equivalent to the multiplicative objective. Be aware that the magnitude of `total_loss` is in log-scale and not directly comparable to typical loss values (since it’s the log of a product), but for optimization purposes that doesn’t matter. Also, monitor if any loss becomes extremely small or large; if \$\log(L\_i)\$ starts diverging (to \$-\infty\$ or \$+\infty\$), that indicates numerical trouble (loss too close to 0 or exploding). If needed, increase \$\epsilon\$ or add gradient clipping.

**Zero Loss Pitfall:** With a product loss, **zero is a singular point**. If at any time one loss term becomes exactly 0, the product is 0 and the total loss is minimized – but the model might not have actually learned the other objectives. This is usually not a concern for well-behaved losses (e.g. \$L\_2\$ or perceptual losses rarely hit exact 0 unless the fit is perfect), but one should be mindful. The log-sum formulation handles this more gracefully: as \$L\_i \to 0\$, \$\log(L\_i)\$ tends to \$-\infty\$, so \$\tilde{L}\$ cannot hit a false optimum just because one term hit zero – instead \$\tilde{L}\$ will continue to decrease (and the gradient \$1/L\_i\$ will blow up). Thus, the log form prevents the trivial solution of “drive one loss to exactly zero and stop,” which a naive product might erroneously consider optimal (gradient zero in all terms due to multiplication by zero).

In summary, for practical training, if you wish to experiment with multiplicative loss combination, it is **highly recommended** to optimize the sum of log-losses (geometric mean) rather than directly multiplying raw loss values. This approach maintains the spirit of the product (all terms must be small) while avoiding vanishing gradients. Indeed, a computer vision study proposing a geometric mean loss noted that taking the log (summing logs) is equivalent to “normalizing individual losses and then adding them” – in other words, each loss is implicitly scaled to a comparable level each iteration. They opted for the raw product in implementation for simplicity, but acknowledged the log interpretation. In PyTorch, both approaches (raw product vs. log-sum) are implementable, but the log-sum is more numerically robust. If using raw product (`total_loss = loss1 * loss2 * loss3`), consider monitoring its magnitude and maybe adding a very small constant to each loss to avoid a zero product.

## Examples of Multiplicative Loss Usage

While less common than additive losses, **multiplicative loss combinations have been explored in various contexts**. Below are a few illustrative examples and domains where the product or geometric mean of losses has been used successfully (or at least instructively):

### Style Transfer with Multiplicative Content+Style Loss

Neural style transfer is a task that traditionally uses a weighted sum of a *content reconstruction loss* and a *style reconstruction loss*. Typically, one must choose a weight to trade off content fidelity vs. style adherence. In a 2018 paper by Yeh and Tang, the authors discovered that using a **multiplicative combination of the style and content losses** yields better results than the usual additive approach. They reported that *“multiplicative, rather than additive, style and content loss, results in very good style transfer”*, with a visible emphasis on image boundaries and the advantage of removing one hyper-parameter (the content-vs-style weight). In essence, setting \$L\_{\text{total}} = L\_{\text{content}} \times L\_{\text{style}}\$ forced the optimization to simultaneously satisfy content and style requirements, instead of allowing a compromise where one could be partially neglected. The multiplicative loss gave **sharper stylization** and ensured neither content nor style loss blew up, since the product penalizes large values in either. This is a concrete computer vision example where a multiplicative loss proved effective and simplified parameter tuning. (Internally, one can imagine they effectively optimized \$\log L\_{\text{content}} + \log L\_{\text{style}}\$ – which automatically balances the two objectives.) Notably, their approach eliminated the need to manually decide the weighting between style and content; the tradeoff became implicit in the optimization. This suggests that for certain problems with two competing objectives, a product loss can naturally find a balance point that an additive loss would have needed careful weight adjustment to achieve.

### Multi-Task Learning with Geometric Mean Loss

In multi-task learning (MTL), where a single network learns several tasks (each with its own loss), choosing task weights is a major challenge. A recent line of research has looked at **geometric mean loss strategies** to address this. For example, Chennupati et al. (CVPR 2019 workshop) proposed using the **geometric mean of task losses as a better alternative to a weighted average** in autonomous driving tasks. If \$L\_1, L\_2, ..., L\_n\$ are the task-specific losses (e.g. one for segmentation, one for depth, one for motion), their total loss was defined as \$(L\_1 L\_2 \cdots L\_n)^{1/n}\$ (the \$n\$th root keeps the scale comparable to individual losses). They found this **“facilitates better handling of differences in convergence rates of different tasks”**. The rationale is that the geometric mean penalizes imbalance: if any one task loss remains high, the geometric mean stays high, even if others are low. This prevents the model from over-focusing on one task and nearly ignoring another. Their experiments on datasets like KITTI and Cityscapes showed that this strategy led to more **balanced training and improved performance** on all tasks compared to using a weighted sum of losses. In their words, the geometric mean loss *“makes sure that all tasks are making progress”* during training.

One interesting addition in their approach was a **Focused Loss Strategy (FLS)**, where if certain tasks are deemed more important, the loss could include an extra factor for those tasks. Essentially, they multiplied the geometric mean of a subset of “focused” task losses into the total objective to give those tasks extra attention. This is like a hybrid between pure geometric mean and a weighted scheme: it preserves the joint nature of the objective but allows prioritization of some losses. They note that taking the log of the product (which turns it into a sum of logs) can be interpreted as *“equivalent to normalizing individual losses and then adding them”*, although they caution that directly using logs in implementation can be “computationally complex” (likely referring to the need to handle gradients carefully). In practice, they optimized the product directly and achieved good results. The follow-up “NeurAll” project (2020) reinforced this finding, demonstrating that a product-of-losses approach outperformed equal-weight summation for both 2-task and 3-task models in vision, achieving similar accuracies to single-task models without tuning any task weights.

These successes in MTL suggest that multiplicative loss can be a viable solution to auto-balance tasks. However, they also highlight the **numerical sensitivity**: one paper mentions that the geometric mean method *“does not require any additional hyperparameters, but it is numerically sensitive to a large number of tasks”*. If you have many tasks, the product of many losses may become very small or difficult to handle (again, the log-sum trick helps, but if there are, say, 50 tasks, the product of 50 terms might be extremely tiny and the log-sum might amplify slight estimation errors). So in multi-task scenarios with a moderate number of tasks (a handful), geometric mean can be great; with very many tasks, one might need to combine it with other techniques or fall back to weighted sums.

### Other Domains – Fairness Metric Example

Outside of traditional vision tasks, multiplicative objectives have appeared in areas like algorithmic fairness. In a U.S. recidivism prediction challenge (NIJ 2021), the performance metric (to be maximized) was defined as **\$(1 - \text{MSE}) \times (1 - |\text{FP}*\text{Black} - \text{FP}*\text{White}|)\$**, where MSE is the prediction error and the second term is a fairness penalty (the absolute difference in false positive rates between Black and White groups). This is effectively a multiplicative *loss* (with two components: accuracy and fairness). The competition designers chose multiplication over a weighted sum of error + fairness penalty, perhaps to ensure that solutions had to be both accurate *and* fair (a large error or large disparity would both sink the score). Interestingly, researchers analyzing this metric found that it led to trivial solutions: optimizing the product encouraged models that simply predicted “no recidivism” for everyone, which gave near-optimal fairness and acceptable MSE. In this case, the multiplicative formulation **unintentionally allowed a degenerate optimum** – because one factor (fairness) could be maximized by a trivial strategy that incidentally didn’t hurt the other factor too much, the product objective was satisfied. An additive metric might have penalized error and fairness separately in a way that this trivial solution wouldn’t score as highly. This illustrates that multiplicative metrics can have their own quirks: they are *not linear*, so a small improvement in one factor can compensate for a big degradation in another if the values allow (due to the product structure). In the NIJ example, the product metric was nearly as high for a trivial predictor as for more sophisticated ones, showing the difficulty of designing a perfect combined objective.

The takeaway is that multiplicative loss usage is context-dependent. In style transfer and multi-task vision, it showed clear benefits and simplified hyperparameters. In the fairness scenario, it had unintended consequences (though that was a metric for evaluation rather than a trainable loss – still, one could imagine training a model to directly maximize that metric, encountering similar issues). Other domains like reinforcement learning occasionally use multiplicative factors (e.g. combining a main objective with a penalty by multiplication), but it’s not widespread. When considering a multiplicative loss, one should carefully analyze whether the combined objective truly reflects the desired tradeoff. Often, a slight modification or a constraint is needed to avoid degenerate solutions.

## Hybrid and Alternative Combination Strategies

Beyond the pure sum or pure product of losses, researchers have explored **hybrid approaches** and other mathematical means to combine multiple objectives. The goal is usually to capture the benefits of both additive and multiplicative schemes without their downsides. Some notable strategies include:

* **Geometric Mean (Log-Sum) of Losses:** As discussed earlier, the geometric mean \$((\prod\_i L\_i)^{1/n})\$ is effectively a multiplicative strategy implemented via an additive log domain. This approach has gained traction because it requires *no manual weighting* and inherently balances tasks. It can be seen as a hybrid of sorts: it’s a product at heart, but optimization is done by adding log-losses (which is similar to an adaptive weighting each step). The geometric mean is a **special case of a power mean** – specifically, it’s the \$p\to0\$ case of the power mean \$M\_p(L\_1,\ldots,L\_n) = (\frac{1}{n}\sum L\_i^p)^{1/p}\$. By choosing different values of \$p\$, one can interpolate between strategies:

  * \$p=1\$ gives the **arithmetic mean** (sum) \$\frac{1}{n}\sum L\_i\$.
  * \$p=0\$ in the limit gives the **geometric mean** \$\prod L\_i^{1/n}\$.
  * \$p \to -\infty\$ gives the **minimum** (or one could use \$\min(L\_i)\$ directly).
  * \$p \to +\infty\$ gives the **maximum**.
  * Negative \$p\$ yields means closer to the minimum – e.g. the **harmonic mean** is \$p=-1\$ case: \$(\frac{1}{n}\sum \frac{1}{L\_i})^{-1}\$.

  By selecting \$p\$, one can set how strongly the combined loss punishes imbalance. The harmonic mean (which is \$\frac{n}{\sum 1/L\_i}\$) heavily weights the smallest values – in the context of losses, that means it **emphasizes the smallest loss** (which is counter-intuitive, since a small loss means that task is doing well). In fact, if one loss goes to 0, the harmonic mean also goes to 0, even if other losses are high – so harmonic mean is *not* a sensible objective for multi-loss training (it would consider the job done if one task is perfected, regardless of others). The harmonic mean is more useful when combining **rates or accuracies**, not losses, so we rarely use it for loss combination. The geometric mean, by contrast, doesn’t overly reward one term being extremely small unless the others are also small – it’s a balanced middle ground. And the arithmetic mean (sum) is linear, treating them in proportion.

  Some researchers have treated the **task of loss combination as finding the right power mean**: an \$\alpha\$-mean where \$\alpha\$ could even vary during training. In practice, the geometric mean has emerged as a reasonable compromise – it ensures none of the losses can be too large or too small relative to the others if the overall objective is to decrease. One must be cautious that even geometric mean can be “numerically sensitive” if the number of losses is large (because it multiplies many terms). For up to, say, a dozen tasks it can work; for hundreds of tasks, one might need a different strategy.

* **Maximizing the Worst-Case (Min-Max Loss):** Another alternative is to formulate the objective as minimizing the maximum loss: \$L\_{\text{max}} = \max{L\_1, L\_2, ..., L\_n}\$. This is a **minimax approach** where the goal is to drive the worst-performing metric down, which inherently balances because once the highest loss decreases enough, another loss might become the highest, etc. This approach is sometimes seen in multi-objective optimization research and has connections to fairness (treat each loss equally by focusing on the worst). However, \$L\_{\text{max}}\$ is non-differentiable (the maximum operation has a kink where the argmax switches). One can approximate it with a smooth softmax: e.g. using a **softmax combination** \$L\_{\text{softmax}} = \frac{\sum L\_i \exp(k L\_i)}{\sum \exp(k L\_i)}\$ which for large \$k\$ approximates \$\max L\_i\$. This way, gradients flow to the largest loss more strongly. While conceptually appealing, directly minimizing the max loss can lead to oscillations (the identity of the max might switch frequently between tasks), and it does not allow trade-offs – the model will sacrifice everything to equalize losses. In some cases (like strict requirements that all criteria meet a threshold), this could be appropriate, but in typical tasks we do allow some trade-off, so this is not commonly used.

* **Harmonic Mean (or Other Means):** As mentioned, harmonic mean is generally unsuitable for loss objectives because it *de-emphasizes* large values. To illustrate, if one loss \$L\_1\$ is huge (worst case), harmonic mean \$H = \frac{n}{\frac{1}{L\_1}+\cdots+\frac{1}{L\_n}}\$ will be dominated by \$1/L\_1 \approx 0\$, so \$H \approx \frac{n}{\text{(sum of other terms)}}\$ – basically ignoring the large loss. That’s opposite of what we want (we want to penalize large losses more, not less). Therefore, harmonic or other *lower*-order means are not used for combining loss terms.

* **Convex Combinations / Weighted Averages in a Schedule:** A simple hybrid is to start training with one combination and finish with another. For instance, one might initially use an additive loss (to get stable training) and later transition to a multiplicative or geometric loss (to enforce strict convergence on all tasks). This could address the vanishing gradient issue: early training gets the losses down to a certain range, then the multiplicative loss takes over to fine-tune balance. However, such schedules are heuristic and need careful tuning themselves.

* **Task-Specific Normalization:** Another approach is to dynamically normalize each loss by some measure of its scale. For example, one method is to divide each loss by its initial value or by an exponentially weighted moving average of its recent values. This doesn’t change the overall form (it’s still a weighted sum effectively), but it ensures all losses start at roughly comparable scale and remain so. An example provided in a blog uses the **reciprocal of the current loss value** as a weight for that loss. In PyTorch pseudocode:

  ```python
  combined_loss = L1 / (L1.detach() + \epsilon) + L2 / (L2.detach() + \epsilon) + L3 / (L3.detach() + \epsilon)
  ```

  Here `.detach()` is used to treat the current loss values as constants (no gradient flow through those), so effectively at each step each loss \$L\_i\$ is weighted by \$\frac{1}{L\_i}\$ (plus a tiny epsilon to avoid division by zero). This makes each normalized term roughly 1 before backprop (because \$L\_i/(L\_i\$ (detached)\$) \approx 1\$), meaning **each loss contributes equally in magnitude** to the total, regardless of scale. The gradients, however, are not all zero because the \$\text{detach}\$ stops the denominator from affecting the gradient. So the gradient w\.r.t. network parameters is \$\nabla\_\theta (L\_i/L\_i\text{(detached)}) = \frac{1}{L\_i\text{(detached)}} \nabla\_\theta L\_i\$. This is effectively the same principle as the log-sum (each gradient gets weighted by \$1/L\_i\$). Many experiments have found this simple normalization trick to be a strong baseline for multi-loss training – it’s easy to implement and often balances tasks reasonably well without any fancy algorithm. It’s not a fundamentally new objective, just a dynamic re-weighting applied to the sum.

In summary, beyond sum and product, one can consider **power means, max-based objectives, or dynamic normalizations**. Among these, the **geometric mean (log-sum)** has proven practical and is conceptually a midpoint between add and multiply. It’s been explicitly used under names like *“geometric loss strategy (GLS)”*, noted for not requiring hyper-parameters. Other dynamic schemes (which we discuss next) can be viewed as sophisticated ways to adjust the additive weights over time, achieving a result somewhat similar to what geometric mean does inherently.

## Modern Methods for Loss Balancing and Weighting

Selecting and tuning loss weights is such a crucial issue in multi-objective training that a variety of **automated loss balancing methods** have been proposed. These methods typically keep the overall objective as a weighted sum, \$L = \sum w\_i(t),L\_i\$, but they *adapt the weights \$w\_i(t)\$ during training* based on certain criteria. The aim is to relieve the user from manual weight tuning and ensure that all tasks/losses learn at a reasonable pace. We briefly highlight a few influential approaches:

* **Uncertainty-Based Weighting:** Kendall et al. (2018) introduced a principled method to derive weights from the **homoscedastic uncertainty** of each task. The idea is grounded in probability theory: if you assume the output of task \$i\$ has an observation noise with variance \$\sigma\_i^2\$, then you can show that the optimal multi-task loss (negative log-likelihood) takes the form
  $L = \sum_i \frac{1}{2\sigma_i^2} L_i + \log(\sigma_i)$
  (for regression tasks; classification yields a similar form). Here \$\sigma\_i\$ becomes a trainable parameter – intuitively, the model can increase \$\sigma\_i\$ if task \$i\$ has inherently high loss/uncertainty, which reduces \$\frac{1}{2\sigma\_i^2}L\_i\$ (down-weighting that loss’s influence). The \$\log(\sigma\_i)\$ term prevents \$\sigma\_i\$ from growing without bound (that term acts as a regularizer). During training, the network learns both the main weights and these \$\sigma\$ parameters via backpropagation. The result is that tasks with higher learned uncertainty (i.e. those that the model finds harder to optimize or noisier) get automatically lower weight, and easier or less noisy tasks get higher weight. This method **accounts for differing units and scales** of losses in a sound way – for example, it can simultaneously handle a depth estimation loss in meters and a classification loss in log-probabilities by giving each an appropriate dynamic weight. The authors reported that this **learned weighting outperformed manual tuning** and even outperformed training each task in isolation. In practice, one can implement this in PyTorch by introducing a small `nn.Parameter` for each \$\sigma\_i\$ (often optimizing \$\log \sigma\_i\$ for numerical stability), and constructing the loss as shown in the code excerpt. The uncertainty weighting method has become a popular baseline for multi-task learning, as it is easy to implement and often quite effective when tasks are of different natures. One caveat: if the initial guess of \$\sigma\$ is poor or if a task can overly adapt \$\sigma\$ early, it might temporarily give up on a task (by making \$\sigma\$ huge, effectively zero weight on that loss). Some later works found uncertainty weighting can be sensitive to initialization and might converge to suboptimal weightings (e.g. overfitting the weights). Variants and improvements (such as **Analytical Uncertainty Weighting** which analytically solves for optimal \$\sigma\$ at each step, yielding essentially the inverse-loss weighting we described earlier) have been proposed to address these issues.

* **GradNorm (Gradient Normalization):** GradNorm (Chen et al., 2018) takes a different approach by focusing on the **gradients of each task loss** rather than their uncertainty or magnitude. The idea is to adjust weights such that all tasks train at *similar rates*. Concretely, GradNorm looks at the \$L\_2\$ norm of the gradient of each weighted loss \$w\_i L\_i\$ with respect to the shared network parameters. It then tries to keep these gradient norms in proportion to some target (often the initial loss ratios or a desired training rate). If one task’s gradient norm is too low (meaning that task is learning slower than others), GradNorm will increase that task’s weight \$w\_i\$. If a task’s gradient norm is too high (learning too fast or dominating training), GradNorm will decrease its weight. The method defines a loss \$L\_{\text{grad}} = \sum\_i \big(G\_i(t) - \bar{G}(t) \[r\_i(t)]^{\alpha}\big)^1\$ (where \$G\_i\$ is the gradient norm for task \$i\$, \$\bar{G}\$ is the mean gradient norm, \$r\_i(t)\$ is the ratio of current loss to initial loss for task \$i\$, and \$\alpha\$ is a hyperparameter controlling how strongly to enforce equal training rates). By minimizing this \$L\_{\text{grad}}\$, the algorithm updates the \$w\_i\$’s (it actually does an extra backward pass to compute the gradient of this meta-loss with respect to the \$w\_i\$ parameters). In simpler terms, GradNorm will dynamically tune the loss weights so that *each task’s relative improvement stays on pace*. If task A’s loss is not decreasing as fast as task B’s (relative to their starting values), GradNorm increases weight on task A to speed it up, and/or lowers weight on task B to decelerate it. Empirically, GradNorm achieved strong results on scenarios like NYU v2 multi-task (depth, segmentation, normals), finding a set of weights that even outperformed an extensive manual grid search. One advantage is that it works without knowing the units or scales of losses – it only looks at gradient magnitudes which are unit-consistent (in a sense, after backprop, everything is in terms of parameter gradients). For PyTorch implementation, GradNorm is a bit involved: you need to compute per-task gradient norms each iteration (possibly using hooks or manual `torch.autograd.grad` calls as in the pseudocode) and then update the weights accordingly. The Medium article snippet provides a clean PyTorch implementation where they keep `w` as an `nn.Parameter` and do an extra backward pass for the GradNorm objective. GradNorm introduces one hyperparameter \$\alpha\$ (which they set to 1.5 in experiments, but it’s not very sensitive as long as it’s in a reasonable range).

* **Dynamic Weight Averaging (DWA):** Proposed by Liu et al. (2019), DWA is a simpler heuristic that updates weights based on the **rate of loss change** for each task. Specifically, at epoch \$t\$, they compute the relative decrease of each loss over the previous epoch and set the weight \$w\_i(t)\$ higher if the loss \$L\_i\$ has not decreased much (meaning task \$i\$ is learning slowly) and lower if the loss has decreased a lot (task learning quickly). In formula, something like \$w\_i(t) = \frac{\exp(v\_i/\tau)}{\sum\_j \exp(v\_j/\tau)}\$ where \$v\_i = \frac{L\_i(t-1)}{L\_i(t-2)}\$ (the ratio of loss from one epoch ago to two epochs ago) and \$\tau\$ is a temperature to control how sharply to differentiate tasks. This way, if task \$i\$’s loss stagnated (ratio \~1), it gets a higher weight; if it dropped significantly (ratio < 1), maybe its weight can be lower now. DWA was found to boost performance in multi-task vision benchmarks by adaptively balancing training time among tasks. It does require smoothing or using epoch-level losses to get a clear signal (using instantaneous batch losses might be too noisy).

* **Impartial Multi-Task Learning (IMTL):** A more recent approach (2020) tries to **learn scalar weights such that the *final* losses are equalized**. It adjusts the gradient of each loss by a factor that would make all losses converge to the same value. Roughly, it multiplies each \$\nabla L\_i\$ by a factor so that if you integrate those, \$L\_i\$ would follow a trajectory to be equal to others. The details are beyond our scope, but one interpretation is that it’s trying to ensure no task is left significantly higher than others at convergence – a bit like a min-max goal but implemented via learning weights.

* **Gradient Blending Methods:** Outside pure weighting, some approaches like **PCGrad**, **GradVac**, **CAGrad** focus on modifying gradients when tasks conflict. For example, PCGrad (Gradient Projection) will project one task’s gradient to be orthogonal to another’s if they conflict, rather than adjusting scalar weights. These are complementary to loss weighting – one could use both in principle. They address a different issue (gradient *direction* conflicts rather than scale of magnitudes). Since the question is about loss weighting, we’ll not delve into these, but it’s worth noting they exist as another layer of handling multi-task optimization.

In practice, **uncertainty weighting and GradNorm are two of the most popular methods** (they were explicitly mentioned in the question, likely for that reason). Both are available in open-source implementations and have been used in many multi-task studies. Uncertainty weighting has the appeal of a probabilistic foundation and simplicity (just add learned variance parameters). GradNorm offers a direct control on training dynamics and can outperform static heuristics.

For a PyTorch-based project involving a generative model with VGG, \$L\_1\$, and LPIPS losses, a practitioner might proceed as follows:

* **Start with Weighted Sum:** This is the baseline. One can set initial weights for the three losses based on their scales (e.g., normalize each loss to be \~1 at start). For instance, if the VGG perceptual loss tends to be around 0.5 and LPIPS around 0.2 and \$L\_1\$ around 0.05 (just hypothetical), one could give inverse proportion weights to equalize them initially. This avoids one term overwhelming due to scale. Many practitioners will try a few sets of weights (like \$\alpha=1,\beta=1,\gamma=1\$ vs. maybe emphasizing perceptual losses more etc.) and see which yields the best visual results. Keep in mind that VGG and LPIPS are both perceptual – they might be somewhat redundant, or complementary in capturing texture vs. global structure. \$L\_1\$ ensures pixel-level accuracy. Their optimal balance might require experimentation.

* **Consider Dynamic Weighting:** If manual tuning is hard or the training shows one loss dropping much faster than others (or one hardly dropping at all), consider enabling a dynamic scheme. For example, implement the **reciprocal loss weighting** (real-time adjustment) by dividing each loss by its current or recent average value. This can be as simple as the code snippet with `.detach()` above. This will continuously rebalance the contributions. Anecdotally, this often stabilizes training when one loss is prone to dominate. The downside is it might conflict with some losses’ semantics (e.g., if one loss goes nearly to zero, its weight spikes and could cause instability; some clipping or momentum in the weight adjustment might help).

* **Try Geometric Mean (Log-Sum):** If the goal is truly to only get a good balance and you don’t want to hassle with weights, try optimizing \$\log L\_{\text{VGG}} + \log L\_{1} + \log L\_{\text{LPIPS}}\$ (with small eps added). This will encourage a solution where none of these losses is significantly larger than the others. Monitor training to ensure it doesn’t stall (if it does, maybe one of the losses is hitting near-zero too fast – in which case, slight regularization or going back to a weighted sum might be needed).

* **Advanced Methods:** For a more principled approach, you could implement **GradNorm**. You would treat the shared generator network’s parameters, compute gradients of each loss w\.r.t. those, and update weights each iteration. This is more coding effort but could yield a nice balance without manual tuning. Alternatively, implement the **uncertainty weighting**: introduce three learned parameters \$\sigma\_{\text{VGG}}, \sigma\_{L1}, \sigma\_{\text{LPIPS}}\` and add the loss terms as \$\frac{1}{2\sigma\_i^2}L\_i + \log \sigma\_i\$. This only adds 3 extra parameters to your model and they will adjust the weights during training. Many find this method convenient and relatively robust – you just have to ensure you interpret the \$\sigma\$ values correctly (they tend to settle such that \$\frac{L\_i}{\sigma\_i^2}\$ are of comparable magnitude).

* **Monitor Individual Losses:** Regardless of method, always keep an eye on each loss term separately during training. If you see one loss flatlining while others decrease, that’s a sign of imbalance. Dynamic methods should mitigate that, but it’s good to verify. With multiplicative or geometric mean training, ensure none of the terms is “left behind” – ideally all decrease together.

* **Task Importance:** If, in your application, some losses are more important than others (e.g. maybe LPIPS is your primary metric of image quality, and \$L\_1\$ is only a secondary constraint), then pure multiplication or equal-weight schemes might not be ideal because they treat all objectives as equally critical. In that case, you may still want a weighted sum or a weighted geometric mean (using exponents) to encode your preference. Modern weighting methods can also incorporate task priority (for instance, you could set an uncertainty prior that one task should have lower noise i.e. higher weight).

In conclusion, combining losses is both an art and a science. **Additive losses with well-chosen weights** remain a reliable choice for most use cases – they are stable and easy to implement. **Multiplicative (geometric) loss** offers an elegant way to demand excellence across all criteria, which can be very useful in certain contexts (ensuring no aspect of the output degrades). However, to use it effectively one should apply the log-sum trick to avoid vanishing gradients and be mindful of any one loss becoming too dominant or too negligible. **Hybrid approaches** like geometric mean essentially do this for you, and have shown promising results in research, particularly in multi-task learning where balancing is otherwise hard. Finally, **adaptive weighting algorithms** like uncertainty-based weighting and GradNorm provide practical tools to automatically balance multiple losses in a PyTorch model. They can often achieve what manual tuning or naive combinations cannot, leading to better overall performance. By leveraging these methods, one can focus more on model design rather than fiddling with loss coefficients, and achieve a model that **learns all aspects of the task effectively**.

**Sources:** The analysis above is informed by literature and practical insights on multi-loss training, including Reddit discussions on loss multiplication vs addition, the MultiNet++ study on geometric mean loss, style transfer improvements with multiplicative loss, the NIJ fairness competition metric, and multi-task weighting methods from Kendall et al., GradNorm, and others. These demonstrate the theoretical differences in gradient behavior, report successful (and cautionary) cases of multiplicative losses, discuss stability fixes (log-sum), propose alternative combination strategies, and exemplify modern best practices for loss balancing in deep learning.

